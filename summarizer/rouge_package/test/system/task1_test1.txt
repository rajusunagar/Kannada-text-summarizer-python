Reference summaries are usually written by humans and it has been shown that using multiple reference summaries yields in more reliable ROUGE scores than using just one reference summary.
Basics of Setting up ROUGE Toolkit for Evaluation of Summarization Tasks I have been trying to use the ROUGE Perl toolkit to evaluate one of my research projects but have been finding it really hard to get proper documentation on its usage.
Note that ROUGE can handle any number of peer summaries (if generated by multiple systems) and any number of model summaries.
One is the system generated summaries that is referred to as 'peer summaries' and then you have the reference summaries or gold standard summaries known as 'model summaries'.
Actually, I learnt the basics of using ROUGE from the MEAD documentation! If you have successfully installed ROUGE and need to set up the evaluation mechanism, read on.